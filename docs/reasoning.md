# Reasoning Effort

Opal supports configuring reasoning effort (extended thinking) for models that support it. This controls how much internal reasoning the model performs before responding ‚Äî higher effort means more thorough analysis at the cost of latency and tokens.

## Supported Levels

| Level | Description |
|-------|-------------|
| `off` | No extended thinking (default) |
| `low` | Minimal reasoning ‚Äî fast, economical |
| `medium` | Balanced reasoning and speed |
| `high` | Thorough reasoning ‚Äî slower, more tokens |
| `max` | Unconstrained reasoning (Opus 4.6+ only) |

Supported levels are discovered per-model via LLMDB. Not all models support reasoning ‚Äî the `models/list` RPC returns `supports_thinking` and `thinking_levels` for each model. Most thinking-capable models expose `["low", "medium", "high"]`; Opus 4.6+ models also include `"max"`.

## How It Works Across Providers

```mermaid
graph TD
    User["User selects<br/>model + thinking level"]
    Model["Opal.Model<br/><small>thinking_level: :high</small>"]
    Router{"Provider<br/>router"}
    Copilot["Provider.Copilot"]
    LLM["Provider.LLM"]
    ChatAPI["Chat Completions API<br/><small>reasoning_effort: high</small><br/>‚ö†Ô∏è No visible thinking returned"]
    RespAPI["Responses API<br/><small>reasoning.effort: high<br/>reasoning.summary: auto</small><br/>‚úÖ Reasoning content returned"]
    ReqLLM["ReqLLM<br/><small>reasoning_effort: :high</small>"]
    Anthropic["Anthropic API<br/><small>thinking.type: adaptive<br/>output_config.effort: high</small><br/>‚úÖ Thinking blocks returned"]
    OpenAI["OpenAI API<br/><small>reasoning.effort: high</small><br/>‚úÖ Reasoning content returned"]

    User --> Model
    Model --> Router
    Router -->|":copilot"| Copilot
    Router -->|"other"| LLM
    Copilot -->|"Claude (param sent,<br/>no thinking returned)"| ChatAPI
    Copilot -->|"GPT-5 family"| RespAPI
    LLM --> ReqLLM
    ReqLLM -->|Anthropic| Anthropic
    ReqLLM -->|OpenAI| OpenAI
```

### Copilot Provider

The Copilot API proxies multiple model families through two API variants. Reasoning effort is sent in the request body for thinking-capable models, though **not all models return visible reasoning content** through the proxy.

> **Known limitation (as of Feb 2025):** The Copilot proxy does **not** return `reasoning_content` for Claude models. The `reasoning_effort` parameter is silently accepted (200 OK) but no thinking tokens appear in the response stream. This affects all Claude models (Sonnet 4, Opus 4.5, etc.) regardless of parameter combinations tried ‚Äî including Anthropic-native `thinking.type` formats. The o3/o4 model families are also not available through the proxy. Only **GPT-5 family** models produce visible reasoning output via the Responses API path. This is confirmed by reports from other tools (opencode#6864). The parameter may still influence response quality even without visible thinking.

**Chat Completions** (Claude): Adds `reasoning_effort` string to the request body. No visible thinking is returned.

```elixir
# In Provider.Copilot ‚Äî delegates to shared OpenAI module
defp maybe_add_reasoning_effort(body, %{thinking_level: level, id: id}) do
  if supports_thinking?(id) do
    Map.put(body, :reasoning_effort, Opal.Provider.OpenAI.reasoning_effort(level))
  else
    body
  end
end
```

**Responses API** (GPT-5 family): Adds `reasoning` object with effort level and `summary: "auto"` (required to get reasoning summary text in the stream). Also switches system role to `"developer"`. **This is the only Copilot path that produces visible reasoning content.**

```elixir
# In Provider.Copilot ‚Äî Responses API reasoning
defp maybe_add_reasoning_config(body, %{thinking_level: :off}), do: body
defp maybe_add_reasoning_config(body, %{thinking_level: level}) do
  Map.put(body, :reasoning, %{effort: thinking_level_to_effort(level), summary: "auto"})
end
```

**Level mapping** (via `Opal.Provider.OpenAI.reasoning_effort/1`): `:low` ‚Üí `"low"`, `:medium` ‚Üí `"medium"`, `:high` ‚Üí `"high"`, `:max` ‚Üí `"high"` (clamped ‚Äî the Copilot proxy doesn't support `"max"` natively; use the direct LLM provider for Anthropic's adaptive `"max"`).

**Thinking-capable models** are detected by model ID prefix: `gpt-5*`, `claude-sonnet-4*`, `claude-opus-4*`, `claude-haiku-4.5`, `o3*`, `o4*`. Other models (e.g. `gpt-4o`, `gpt-4.1`) don't receive reasoning params.

**Copilot thinking support matrix:**

| Model Family | API Path | Reasoning Sent? | Visible Thinking? |
|-------------|----------|----------------|-------------------|
| GPT-5, GPT-5.1, GPT-5.2, GPT-5.3 | Responses API | ‚úÖ `reasoning.effort` | ‚úÖ `reasoning_summary_text.delta` events |
| Claude Sonnet 4, Opus 4.5+ | Chat Completions | ‚úÖ `reasoning_effort` | ‚ùå Silently ignored by proxy |
| o3, o4 | Chat Completions | N/A | N/A ‚Äî not available on proxy |
| GPT-4o, GPT-4.1 | Chat Completions | ‚ùå Not sent | ‚ùå Not supported |

Thinking output is parsed from SSE into Opal's semantic events:

| SSE Data | Opal Event |
|----------|------------|
| `{"choices": [{"delta": {"reasoning_content": "..."}}]}` | `{:thinking_delta, text}` |
| `{"type": "response.output_item.added", "item": {"type": "reasoning"}}` | `{:thinking_start, %{item_id: id}}` |
| `{"type": "response.reasoning_summary_text.delta", "delta": "..."}` | `{:thinking_delta, text}` |

### LLM Provider (Direct)

For direct provider access via ReqLLM, the thinking level maps to `reasoning_effort`:

```elixir
# In Provider.LLM
defp maybe_add_thinking(opts, %{thinking_level: :off}), do: opts
defp maybe_add_thinking(opts, %{thinking_level: level}) do
  Keyword.put(opts, :reasoning_effort, level)
end
```

ReqLLM then translates per provider:

| Opal Level | Anthropic (Opus 4.6+ adaptive) | Anthropic (older, budget) | OpenAI |
|------------|-------------------------------|---------------------------|--------|
| `:low` | `effort: "low"` | `budget_tokens: 2048` | `reasoning.effort: "low"` |
| `:medium` | `effort: "medium"` | `budget_tokens: 8192` | `reasoning.effort: "medium"` |
| `:high` | `effort: "high"` | `budget_tokens: 16384` | `reasoning.effort: "high"` |
| `:max` | `effort: "max"` | clamped ‚Üí `:high` | clamped ‚Üí `"high"` |

## Thinking Persistence and Roundtripping

Thinking content is **persisted in messages** via the `thinking` field on `Opal.Message`. When the agent finalizes a response, accumulated thinking text is stored alongside the assistant message content and tool calls.

```mermaid
graph LR
    Stream["SSE Stream"] --> Accumulate["stream.ex<br/><small>current_thinking</small>"]
    Accumulate --> Finalize["agent.ex<br/><small>Message.assistant(text, calls, thinking: ...)</small>"]
    Finalize --> Session["session.ex<br/><small>JSONL persistence</small>"]
    Finalize --> NextTurn["Next API call<br/><small>Thinking included in<br/>message conversion</small>"]
```

**Why roundtrip?** OpenAI's docs recommend passing back reasoning items between tool calls to maintain reasoning continuity. Anthropic's thinking blocks have cryptographic signatures for context continuity. Without roundtripping, the model loses its chain of thought at every tool call.

**How it roundtrips per API variant:**

- **Chat Completions**: Previous assistant messages include `reasoning_content` field if they had thinking content.
- **Responses API**: A `reasoning` item with `summary_text` is prepended to the assistant's output items in the input array.

The `stream.ex` handler auto-emits a `thinking_start` event before the first `thinking_delta` if the provider didn't send one (Chat Completions doesn't emit explicit start events). This is detected via `current_thinking` being `nil` (not started) vs `""` (started).

## CLI Display

Thinking content is displayed in the CLI timeline when **Ctrl+O** is toggled on (the same toggle that shows tool output):

- Thinking blocks appear as `{ kind: "thinking", text: string }` entries in the timeline, interspersed with messages and tool calls in the order they occurred.
- Rendered with a `üí≠` prefix in dim/italic gray to distinguish from assistant text.
- Truncated to 8 lines when viewing historical thinking blocks.
- During streaming, the animated `ThinkingIndicator` kaomoji still appears below the timeline.

When Ctrl+O is off (default), thinking entries are hidden from the timeline view.

## Setting Reasoning Effort

### Via Model Picker (CLI)

The `/models` command opens an interactive picker. After selecting a model that supports thinking, a second picker appears with the available thinking levels (fetched from the server, not hardcoded).

### Via RPC

```json
// Set model with thinking level
{"method": "model/set", "params": {
  "session_id": "...",
  "model_id": "claude-opus-4.6",
  "thinking_level": "high"
}}

// Change thinking level without switching models
{"method": "thinking/set", "params": {
  "session_id": "...",
  "level": "medium"
}}
```

### Via Elixir API

```elixir
# At session start (thinking_level flows through Session.Builder)
Opal.start_session(%{
  model: {:copilot, "claude-opus-4.6"},
  thinking_level: :high
})

# Mid-session
model = Opal.Model.new(:copilot, "claude-opus-4.6", thinking_level: :high)
GenServer.call(agent, {:set_model, model})
```

## Model Discovery

The `models/list` RPC returns reasoning capability per model:

```json
{
  "models": [
    {
      "id": "claude-opus-4.6",
      "name": "Claude Opus 4.6",
      "provider": "copilot",
      "supports_thinking": true,
      "thinking_levels": ["low", "medium", "high", "max"]
    },
    {
      "id": "claude-sonnet-4.5",
      "name": "Claude Sonnet 4.5",
      "provider": "copilot",
      "supports_thinking": true,
      "thinking_levels": ["low", "medium", "high"]
    },
    {
      "id": "gpt-4o",
      "name": "GPT-4o",
      "provider": "copilot",
      "supports_thinking": false,
      "thinking_levels": []
    }
  ]
}
```

This data comes from LLMDB's `reasoning.enabled` capability flag. Opus 4.6+ models get the additional `"max"` level. The CLI model picker uses this to decide whether to show the thinking level step.

## Event Flow

```
Provider SSE ‚Üí parse_stream_event/1 ‚Üí {:thinking_start, %{}}
  (Copilot)                          ‚Üí {:thinking_delta, "Let me analyze..."}

Provider EventStream ‚Üí {ref, {:events, [{:thinking_delta, "..."}]}}
  (LLM/ReqLLM)         (no JSON round-trip)

             ‚Üí stream.ex             ‚Üí Accumulates into current_thinking
                                      ‚Üí Auto-emits thinking_start if missing
             ‚Üí Agent broadcasts      ‚Üí {:thinking_start}
                                      ‚Üí {:thinking_delta, %{delta: "..."}}
             ‚Üí RPC serializes        ‚Üí {"type": "thinking_start"}
                                      ‚Üí {"type": "thinking_delta", "delta": "..."}
             ‚Üí CLI reducer           ‚Üí Timeline: {kind: "thinking", text: "..."}
                                      ‚Üí AgentView.thinking: "..." (indicator)
             ‚Üí finalize_response     ‚Üí Message.assistant(text, calls, thinking: "...")
                                      ‚Üí Persisted to session JSONL
```

## Source

- `core/lib/opal/model.ex` ‚Äî `thinking_level` field and validation (`:off | :low | :medium | :high | :max`)
- `core/lib/opal/models.ex` ‚Äî Per-model `thinking_levels` from LLMDB, `supports_max_thinking?/1`
- `core/lib/opal/message.ex` ‚Äî `thinking` field on Message struct
- `core/lib/opal/provider/openai.ex` ‚Äî Shared `parse_chat_event/1`, `convert_messages/2`, `reasoning_effort/1`
- `core/lib/opal/provider/copilot.ex` ‚Äî `maybe_add_reasoning_effort/2`, `maybe_add_reasoning_config/2`, `supports_thinking?/1`, Responses API conversion
- `core/lib/opal/provider/llm.ex` ‚Äî `maybe_add_thinking/2` ‚Üí ReqLLM `reasoning_effort`, thinking roundtrip via metadata
- `core/lib/opal/provider/event_stream.ex` ‚Äî EventStream struct for native event delivery
- `core/lib/opal/agent/stream.ex` ‚Äî Thinking accumulation and auto-start detection
- `core/lib/opal/agent.ex` ‚Äî `current_thinking` state, dual `handle_info` (SSE + EventStream), `finalize_response`
- `core/lib/opal/session.ex` ‚Äî Thinking persistence in JSONL
- `core/lib/opal/rpc/handler.ex` ‚Äî `thinking/set` and `model/set` with `thinking_level`
- `cli/src/hooks/use-opal.ts` ‚Äî Timeline thinking entries, `appendThinkingDelta`
- `cli/src/components/message-list.tsx` ‚Äî `ThinkingBlock` component
- `cli/src/components/model-picker.tsx` ‚Äî Two-step picker (model ‚Üí thinking level)
- `core/test/opal/reasoning_effort_test.exs` ‚Äî Reasoning effort unit tests
- `core/test/opal/thinking_integration_test.exs` ‚Äî Full-stack thinking integration tests (fixture replay)
- `core/test/opal/live_thinking_test.exs` ‚Äî Live API tests that record thinking fixtures
- `core/test/opal/provider/openai_test.exs` ‚Äî Shared OpenAI module tests

## References

- [OpenAI Reasoning Guide](https://developers.openai.com/api/docs/guides/reasoning) ‚Äî Official docs for `reasoning.effort` and `reasoning.summary` parameters on the Responses API.
- [Anthropic Extended Thinking](https://platform.claude.com/docs/en/build-with-claude/extended-thinking) ‚Äî Official docs for budget-based and adaptive thinking modes, including `output_config.effort` levels.
- [opencode#6864](https://github.com/anomalyco/opencode/issues/6864) ‚Äî Confirms the Copilot proxy does not return `reasoning_content` for Claude models. Other tools experience the same limitation.
